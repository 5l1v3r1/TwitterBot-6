{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation bible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). \n",
    "\n",
    "also on my udacity course in artifical intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_io_pairs(text,window_size,step_size):\n",
    "    # number of unique chars\n",
    "    chars = sorted(list(set(text)))\n",
    "    num_chars = len(chars)\n",
    "    \n",
    "    # cut up text into character input/output pairs\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(window_size, len(text), step_size):\n",
    "        inputs.append(text[(i-window_size):i])\n",
    "        outputs.append(text[i:(i+1)])\n",
    "    \n",
    "    # create empty vessels for one-hot encoded input/output\n",
    "    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n",
    "    \n",
    "    # loop over inputs/outputs and transform and store in X/y\n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, chars_to_int[char]] = 1\n",
    "        y[i, chars_to_int[outputs[i]]] = 1\n",
    "        \n",
    "    return X,y\n",
    "\n",
    "\n",
    "def random_primer(primer_dict):\n",
    "    \n",
    "    chapter = random.choice(list(primer_dict.keys()))\n",
    "    \n",
    "    line = int(primer_dict[chapter]) + 1\n",
    "    sub_chapter =  random.randint(line,line+50)\n",
    "    \n",
    "    return \"{}:{} \".format(chapter, sub_chapter)\n",
    "\n",
    "\n",
    "# function that uses trained model to predict a desired number of future characters\n",
    "def predict_next_chars(model,num_chars,input_chars,num_to_predict):     \n",
    "    # create output\n",
    "    predicted_chars = ''\n",
    "    for i in range(num_to_predict):\n",
    "        # convert this round's predicted characters to numerical input    \n",
    "        x_test = np.zeros((1, window_size, num_chars))\n",
    "        for t, char in enumerate(input_chars):\n",
    "            x_test[0, t, chars_to_int[char]] = 1.\n",
    "\n",
    "        # make this round's prediction\n",
    "        test_predict = model.predict(x_test,verbose = 0)[0]\n",
    "\n",
    "        # translate numerical prediction back to characters\n",
    "        r = np.argmax(test_predict)                           # predict class of each test input\n",
    "        d = int_to_chars[str(r)] \n",
    "\n",
    "        # update predicted_chars and input\n",
    "        predicted_chars+=d\n",
    "        input_chars+=d\n",
    "        input_chars = input_chars[1:]\n",
    "    return predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/steffen/Documents/RobotBible/luther_bibel_1912.txt', 'r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text has 4338574 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Gen 1:1 Am Anfang schuf Gott Himmel und Erde.\\nGen 1:2 Und die Erde war wüst und leer, und es war finster auf der Tiefe; und der Geist Gottes schwebte auf dem Wasser.\\nGen 1:3 Und Gott sprach: Es werde Licht! und es ward Licht.\\nGen 1:4 Und Gott sah, dass das Licht gut war. Da schied Gott das Licht von der Finsternis\\nGen 1:5 und nannte das Licht Tag und die Finsternis Nacht. Da ward aus Abend und Morgen der erste Tag.\\nGen 1:6 Und Gott sprach: Es werde eine Feste zwischen den Wassern, und die sei ein Unterschied zwischen den Wassern.\\nGen 1:7 Da machte Gott die Feste und schied das Wasser unter der Feste von dem Wasser über der Feste. Und es geschah also.\\nGen 1:8 Und Gott nannte die Feste Himmel. Da ward aus Abend und Morgen der andere Tag.\\nGen 1:9 Und Gott sprach: Es sammle sich das Wasser unter dem Himmel an besondere Örter, dass man das Trockene sehe. Und es geschah also.\\nGen 1:10 Und Gott nannte das Trockene Erde, und die Sammlung der Wasser nannte er Meer. Und Gott sah, dass es gut war'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('text has ' + str(len(text)) + ' characters')\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R', '<', 'u', 'H', 's', '„', 'f', '5', ':', '\\n', 'O', '1', 'B', 'o', 'N', ']', 'C', ' ', '{', 'U', 'h', 'Q', 'ß', '2', 'd', '3', 'D', '–', 'Ä', 'g', '>', \"'\", 'b', 'i', 'L', '.', '0', 'a', 'v', 'M', '”', 'r', 'Z', 'J', 'ö', 'c', '?', '[', 'A', '8', '’', '-', 'K', 'l', 'w', '´', 'E', 'V', '}', '‚', '(', 'e', 'y', '6', 'ä', '4', '9', ';', ')', 'm', 't', '!', 'ü', 'P', 'z', 'k', 'j', 'x', 'p', ',', 'Ü', 'T', 'Ö', 'G', '7', 'F', 'W', 'S', 'q', 'I', 'n'}\n"
     ]
    }
   ],
   "source": [
    "print(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "text = text.replace('\"', \"'\")\n",
    "text = text.replace(\"”\", \"'\")\n",
    "text = text.replace(\"„\",\"'\")\n",
    "text = text.replace(\"’\",\"'\")\n",
    "text = text.replace(\"´\",\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = ['!', ',', '.', ':', ';', '?','\"', '\\n']\n",
    "umlaute = ['ä', 'ö', 'ü', 'ß']\n",
    "\n",
    "text_chars = ''.join(set(text))\n",
    "\n",
    "remain_chars = string.ascii_lowercase + string.digits + ''.join(set(punctuation)) + ''.join(set(umlaute))\n",
    "remove_chars = [i for i in text_chars if i not in remain_chars]\n",
    "\n",
    "for char in remove_chars:\n",
    "    text = text.replace(char, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    text = text.replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this text has 48 unique characters\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "print (\"this text has \" +  str(len(chars)) + \" unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the last char of the text is also a \\n seperator. Let's remove it, for the calculation of the primer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primer = [i.split(':') for i in text.split('\\n')]\n",
    "primer = {i[0]: i[1].split(' ')[0] for i in primer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "step_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(window_size, len(text), step_size):\n",
    "    inputs.append(text[(i-window_size):i])\n",
    "    outputs.append(text[i:(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input =  anfang schuf gott himmel und erde.\n",
      "gen 1:2 und die erde war wüst und leer, und es war finster auf d\n",
      "output = e\n",
      "--------------\n",
      "input = n unterschied zwischen den wassern.\n",
      "gen 1:7 da machte gott die feste und schied das wasser unter der\n",
      "output =  \n"
     ]
    }
   ],
   "source": [
    "print('input = ' + inputs[2])\n",
    "print('output = ' + outputs[2])\n",
    "print('--------------')\n",
    "print('input = ' + inputs[100])\n",
    "print('output = ' + outputs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this corpus has 48 unique characters\n",
      "and these characters are \n",
      "['\\n', ' ', '!', ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ß', 'ä', 'ö', 'ü']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")\n",
    "print ('and these characters are ')\n",
    "print (chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_chars = dict(enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "step_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_chars = len(int_to_chars.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = encode_io_pairs(text,window_size,step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_io_pairs(text,window_size,step_size):\n",
    "    # number of unique chars\n",
    "    chars = sorted(list(set(text)))\n",
    "    num_chars = len(chars)\n",
    "    \n",
    "    # cut up text into character input/output pairs\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(window_size, len(text), step_size):\n",
    "        inputs.append(text[(i-window_size):i])\n",
    "        outputs.append(text[i:(i+1)])\n",
    "    \n",
    "    # create empty vessels for one-hot encoded input/output\n",
    "    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n",
    "    \n",
    "    # loop over inputs/outputs and transform and store in X/y\n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, chars_to_int[char]] = 1\n",
    "        y[i, chars_to_int[outputs[i]]] = 1\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(window_size, num_chars)))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_meta = {}\n",
    "model_meta['text_encoder'] = int_to_chars \n",
    "model_meta['text_decoder'] = chars_to_int\n",
    "model_meta['num_classes'] = num_chars\n",
    "model_meta['primer'] = primer\n",
    "model_meta['model_arch'] = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/model_meta.json', 'w') as output:\n",
    "    json.dump(model_meta, output, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "867421/867421 [==============================] - 3103s 4ms/step - loss: 1.9751\n",
      "Epoch 2/50\n",
      "867421/867421 [==============================] - 3053s 4ms/step - loss: 1.5553\n",
      "Epoch 3/50\n",
      "867421/867421 [==============================] - 3057s 4ms/step - loss: 1.4249\n",
      "Epoch 4/50\n",
      "867421/867421 [==============================] - 3058s 4ms/step - loss: 1.3463\n",
      "Epoch 5/50\n",
      "867421/867421 [==============================] - 3050s 4ms/step - loss: 1.2913\n",
      "Epoch 6/50\n",
      "867421/867421 [==============================] - 3057s 4ms/step - loss: 1.2500\n",
      "Epoch 7/50\n",
      "867421/867421 [==============================] - 3057s 4ms/step - loss: 1.2178\n",
      "Epoch 8/50\n",
      "867421/867421 [==============================] - 3049s 4ms/step - loss: 1.1917\n",
      "Epoch 9/50\n",
      "867421/867421 [==============================] - 3057s 4ms/step - loss: 1.1696\n",
      "Epoch 10/50\n",
      "867421/867421 [==============================] - 3050s 4ms/step - loss: 1.1514\n",
      "Epoch 11/50\n",
      "867421/867421 [==============================] - 3046s 4ms/step - loss: 1.1353\n",
      "Epoch 12/50\n",
      "867421/867421 [==============================] - 3113s 4ms/step - loss: 1.1209\n",
      "Epoch 13/50\n",
      "867421/867421 [==============================] - 3071s 4ms/step - loss: 1.1082\n",
      "Epoch 14/50\n",
      "867421/867421 [==============================] - 3067s 4ms/step - loss: 1.0971\n",
      "Epoch 15/50\n",
      "867421/867421 [==============================] - 3059s 4ms/step - loss: 1.0865\n",
      "Epoch 16/50\n",
      "867421/867421 [==============================] - 3058s 4ms/step - loss: 1.0770\n",
      "Epoch 17/50\n",
      "867421/867421 [==============================] - 3053s 4ms/step - loss: 1.0680\n",
      "Epoch 18/50\n",
      "867421/867421 [==============================] - 3062s 4ms/step - loss: 1.0599\n",
      "Epoch 19/50\n",
      "867421/867421 [==============================] - 3067s 4ms/step - loss: 1.0522\n",
      "Epoch 20/50\n",
      "867421/867421 [==============================] - 3072s 4ms/step - loss: 1.0453\n",
      "Epoch 21/50\n",
      "867421/867421 [==============================] - 3066s 4ms/step - loss: 1.0387\n",
      "Epoch 22/50\n",
      "867421/867421 [==============================] - 3058s 4ms/step - loss: 1.0326\n",
      "Epoch 23/50\n",
      "867421/867421 [==============================] - 3065s 4ms/step - loss: 1.0265\n",
      "Epoch 24/50\n",
      "867421/867421 [==============================] - 3160s 4ms/step - loss: 1.0211\n",
      "Epoch 25/50\n",
      "867421/867421 [==============================] - 3422s 4ms/step - loss: 1.0158\n",
      "Epoch 26/50\n",
      "867421/867421 [==============================] - 3315s 4ms/step - loss: 1.0107\n",
      "Epoch 27/50\n",
      " 89000/867421 [==>...........................] - ETA: 53:38 - loss: 0.9893"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(X, y, batch_size=500, epochs=50,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "# model.save_weights('/Users/steffen/Documents/TwitterBot/model/best_model_weights_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('/Users/steffen/Documents/TwitterBot/model/model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/model_meta.json', 'r', encoding='utf-8') as input:\n",
    "    model_meta = json.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('/Users/steffen/Documents/TwitterBot/model/model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = model_meta['num_classes']\n",
    "int_to_chars = model_meta['text_encoder']\n",
    "chars_to_int = model_meta['text_decoder']\n",
    "primer = model_meta['primer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_chars = {int(k): v for k,v in int_to_chars.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_chars = random_primer(primer)\n",
    "window_size = 100\n",
    "num_to_predict = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TODO_6'></a>\n",
    "\n",
    "With your trained model try a few subsets of the complete text as input - note the length of each must be exactly equal to the window size.  For each subset use the function above to predict the next 100 characters that follow each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.load_weights('/Users/steffen/Documents/TwitterBot/model/best_model_weights_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input = predict_next_chars(model,num_chars, input_chars,num_to_predict = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: choose an input sequence and use the prediction function in the previous Python cell to predict 100 characters following it\n",
    "# get an appropriately sized chunk of characters from the text\n",
    "start_inds = [2001, 3421, 4353, 1]\n",
    "\n",
    "# save output\n",
    "# f = open('text_gen_output/RNN_large_textdata_output.txt', 'w')  # create an output file to write too\n",
    "\n",
    "# load weights\n",
    "model.load_weights('model_weights/best_RNN_large_textdata_weights.hdf5')\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 100)\n",
    "\n",
    "    # print out input characters\n",
    "    line = '-------------------' + '\\n'\n",
    "    print(line)\n",
    "    f.write(line)\n",
    "\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "    f.write(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    predict_line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(predict_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
