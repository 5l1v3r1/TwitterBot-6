{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation bible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). \n",
    "\n",
    "also on my udacity course in artifical intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_io_pairs(text,window_size,step_size):\n",
    "    # number of unique chars\n",
    "    chars = sorted(list(set(text)))\n",
    "    num_chars = len(chars)\n",
    "    \n",
    "    # cut up text into character input/output pairs\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(window_size, len(text), step_size):\n",
    "        inputs.append(text[(i-window_size):i])\n",
    "        outputs.append(text[i:(i+1)])\n",
    "    \n",
    "    # create empty vessels for one-hot encoded input/output\n",
    "    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n",
    "    \n",
    "    # loop over inputs/outputs and transform and store in X/y\n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, chars_to_int[char]] = 1\n",
    "        y[i, chars_to_int[outputs[i]]] = 1\n",
    "        \n",
    "    return X,y\n",
    "\n",
    "\n",
    "def predict_next_chars(model, model_meta, input_chars, window_size,\n",
    "                       num_to_predict):\n",
    "\n",
    "    int_to_chars = model_meta['text_encoder']\n",
    "    chars_to_int = model_meta['text_decoder']\n",
    "\n",
    "    num_chars = len(list(int_to_chars.keys()))\n",
    "\n",
    "    predicted_chars = input_chars\n",
    "\n",
    "    for i in range(num_to_predict):\n",
    "\n",
    "        x_test = np.zeros((1, window_size, num_chars))\n",
    "        for t, char in enumerate(input_chars):\n",
    "            x_test[0, t, chars_to_int[char]] = 1.\n",
    "\n",
    "        test_predict = model.predict(x_test,verbose = 0)[0]\n",
    "\n",
    "        r = np.argmax(test_predict)  # predict class of each test input\n",
    "        d = int_to_chars[str(r)]\n",
    "\n",
    "        # update predicted_chars and input\n",
    "        predicted_chars += d\n",
    "        input_chars += d\n",
    "\n",
    "        if len(input_chars) > window_size:\n",
    "            input_chars = input_chars[1:]\n",
    "\n",
    "    return predicted_chars\n",
    "\n",
    "\n",
    "def extract_verse(chapter_dict, inputs):\n",
    "    \n",
    "    message = inputs.split('\\n')[1]\n",
    "    chapter = message.split(':')\n",
    "    \n",
    "    try:\n",
    "        begin = random_chapter(chapter_dict, chapter[0], False)\n",
    "    except:\n",
    "        begin = random_chapter(chapter_dict, chapter[0], True)\n",
    "    \n",
    "    message = begin + chapter[1][chapter[1].index(' ')+1:]\n",
    "    \n",
    "    return message\n",
    "\n",
    "\n",
    "def random_chapter(primer_dict, chapter, flag):\n",
    "    \n",
    "    if flag:\n",
    "        chapter = random.choice(list(primer_dict.keys()))\n",
    "        \n",
    "    line = int(primer_dict[chapter]) + 1\n",
    "    sub_chapter =  random.randint(line,line+50)\n",
    "    \n",
    "    return \"{}:{} \".format(chapter, sub_chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/steffen/Documents/RobotBible/luther_bibel_1912.txt', 'r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('text has ' + str(len(text)) + ' characters')\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "text = text.replace('\"', \"'\")\n",
    "text = text.replace(\"”\", \"'\")\n",
    "text = text.replace(\"„\",\"'\")\n",
    "text = text.replace(\"’\",\"'\")\n",
    "text = text.replace(\"´\",\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = ['!', ',', '.', ':', ';', '?','\"', '\\n']\n",
    "umlaute = ['ä', 'ö', 'ü', 'ß']\n",
    "\n",
    "text_chars = ''.join(set(text))\n",
    "\n",
    "remain_chars = string.ascii_lowercase + string.digits + ''.join(set(punctuation)) + ''.join(set(umlaute))\n",
    "remove_chars = [i for i in text_chars if i not in remain_chars]\n",
    "\n",
    "for char in remove_chars:\n",
    "    text = text.replace(char, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    text = text.replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "print (\"this text has \" +  str(len(chars)) + \" unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the last char of the text is also a \\n seperator. Let's remove it, for the calculation of the primer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "step_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(window_size, len(text), step_size):\n",
    "    inputs.append(text[(i-window_size):i])\n",
    "    outputs.append(text[i:(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('input = ' + inputs[2])\n",
    "print('output = ' + outputs[2])\n",
    "print('--------------')\n",
    "print('input = ' + inputs[100])\n",
    "print('output = ' + outputs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")\n",
    "print ('and these characters are ')\n",
    "print (chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_chars = dict(enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chapter = [i.split(':') for i in text.split('\\n')]\n",
    "chapter = {i[0]: i[1].split(' ')[0] for i in chapter}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert any texts to escape unjustified claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primer = [i[:100] for i in text.split('\\n')]\n",
    "primer = [[chars_to_int[j] for j in i] for i in primer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "step_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_chars = len(int_to_chars.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = encode_io_pairs(text,window_size,step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(window_size, num_chars)))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_meta = {}\n",
    "model_meta['text_encoder'] = int_to_chars \n",
    "model_meta['text_decoder'] = chars_to_int\n",
    "model_meta['num_classes'] = num_chars\n",
    "model_meta['chapter'] = chapter\n",
    "model_meta['primer'] = primer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/model_meta.json', 'w') as output:\n",
    "    json.dump(model_meta, output, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit(X, y, batch_size=500, epochs=50,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('/Users/steffen/Documents/TwitterBot/model/model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/model_meta.json', 'r', encoding='utf-8') as input:\n",
    "    model_meta = json.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('/Users/steffen/Documents/TwitterBot/model/model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_chars = model_meta['num_classes']\n",
    "int_to_chars = model_meta['text_encoder']\n",
    "chars_to_int = model_meta['text_decoder']\n",
    "primer = model_meta['primer']\n",
    "chapter_dict = model_meta['chapter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert primer back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primer = model_meta['primer']\n",
    "\n",
    "primer = [[int_to_chars[str(j)] for j in i] for i in primer]\n",
    "primer = [''.join(i) for i in primer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_chars = random.choice(primer)\n",
    "window_size = 100\n",
    "num_to_predict = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    input_chars = random.choice(primer)\n",
    "    print(input_chars)\n",
    "    try:\n",
    "        predict_input = predict_next_chars(model,model_meta, input_chars,window_size,500)\n",
    "        print(extract_verse(chapter_dict, predict_input))\n",
    "    except:\n",
    "        continue\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
